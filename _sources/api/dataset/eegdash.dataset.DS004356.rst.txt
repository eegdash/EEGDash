..
   This documentation page is generated during the Sphinx build.
   The underlying code is manually maintained and not autogenerated.

:html_theme.sidebar_secondary.remove:

DS004356
========

*Subcortical responses to music and speech are alike while cortical responses diverge*

Access recordings and metadata through EEGDash.

**Citation:** Tong Shan, Madeline S. Cappelloni, Ross K. Maddox (2022). *Subcortical responses to music and speech are alike while cortical responses diverge*. `10.18112/openneuro.ds004356.v2.2.1 <https://doi.org/10.18112/openneuro.ds004356.v2.2.1>`__

.. rst-class:: sd-badges

:bdg-primary-line:`Modality: eeg` :bdg-secondary-line:`Subjects: 22` :bdg-secondary-line:`Recordings: 639` :bdg-success-line:`License: CC0` :bdg-warning-line:`Source: openneuro` :bdg-info-line:`Citations: 2.0`

:bdg-success:`Metadata: Complete (100%)`

Quickstart
----------

.. tab-set::

   .. tab-item:: Get Started
      :sync: start

      **Install**

      .. code-block:: bash

         pip install eegdash

      **Access the data**

      .. code-block:: python

         from eegdash.dataset import DS004356

         dataset = DS004356(cache_dir="./data")
         # Get the raw object of the first recording
         raw = dataset.datasets[0].raw
         print(raw.info)

   .. tab-item:: Query & Filter
      :sync: query

      **Filter by subject**

      .. code-block:: python

         dataset = DS004356(cache_dir="./data", subject="01")

      **Advanced query**

      .. code-block:: python

         dataset = DS004356(
             cache_dir="./data",
             query={"subject": {"$in": ["01", "02"]}},
         )

      **Iterate recordings**

      .. code-block:: python

         for rec in dataset:
             print(rec.subject, rec.raw.info['sfreq'])

   .. tab-item:: Cite This Dataset
      :sync: cite

      If you use this dataset in your research, please cite the original authors.

      **BibTeX**

      .. code-block:: bibtex

         @dataset{ds004356,
           title = {Subcortical responses to music and speech are alike while cortical responses diverge},
           author = {Tong Shan and Madeline S. Cappelloni and Ross K. Maddox},
           doi = {10.18112/openneuro.ds004356.v2.2.1},
           url = {https://doi.org/10.18112/openneuro.ds004356.v2.2.1},
         }


About This Dataset
------------------


**README**



**Details related to access to the data**


Please contact the following authors for further information:
- Tong Shan (email: tshan@ur.rochester.edu)

.. dropdown:: View full README
   :class-container: sd-shadow-sm

   
   **README**
   
   
   
   **Details related to access to the data**
   
   
   Please contact the following authors for further information:
   - Tong Shan (email: tshan@ur.rochester.edu)
   - Ross K. Maddox (email: rmaddox@ur.rochester.edu)
   
   
   **Overview**
   
   
   The goal of this study is to derive Auditory Brainstem Response (ABR) from continuous music and speech stimuli using deconvolution method. Data collected from Jun to Aug, 2021.
   
   The details of the experiment can be found at Shan et al. (2024). There were two phases in this experiment. For the first phase, ten trials of one-minute clicks were presented to the subjects. For the second phase, the 12 types (six genres of music and six types of speech) of 12 s stimuli clips were presented. There were 40 trials 
   for each type with shuffled order. Between trials, there was a 0.5 s pause. 
   
   The code for stimulus preprocessing and EEG analysis is available on Github: 
   
   https://github.com/maddoxlab/Music_vs_Speech_abr
   
   
   
   **Format**
   
   
   This dataset is formatted according to the EEG Brain Imaging Data Structure. It includes EEG recording from subject 001 to subject 024 (excluding subject 014 and subject 021) in raw brainvision format (including ``.eeg``, ``.vhdr``, and ``.vmrk`` triplet) and stimuli files in format of ``.wav``. 
   
   For some subjects (sub-03 & sub-19), there are 2 "runs" of data that the first run (``run-01``) only contains the click phase (phase 1), and the second run includes the data for the ABR analysis. 
   
   Triggers with values of "1" were recorded to the onset of the stimulus, and shortly after triggers with values of "4" or "8" were stamped to indicate the stimulus types and the trial number out of 40. This was done by converting the decimal trial number to bits, denoted b, then calculating 2 ** (b + 2). Triggers of "999" denote the start of a new segment of EEG. We've specified these trial numbers and more metadata of the events in each of the ``*_eeg_events.tsv`` file, which is sufficient to know which trial corresponded to which type of stimulus and which file.
   
   
   
   **Subjects**
   
   
   24 subjects participated in this study.
   
   **Subject inclusion criteria**
   1. Age between 18-40.
   2. Normal hearing: audiometric thresholds of 20 dB HL or better from 500 to 8000 Hz.
   3. Speak English as their primary language.
   4. Self-reported normal or correctable to normal vision.
   
   **Subject exclusion criteria**
   1. Subject 014 self-withdrew partway through the experiment.
   2. Subject 021 was excluded because of technical problems during data collection that led to unusable data.
   
   Therefore, after excluding the two subjects, there were 22 subjects (11 male and 11 female) with an age of 22.7 ± 5.1 (mean ± SD) years that we included in the analysis. Please see ``subjects.tsv`` for more demography.
   
   
   **Apparatus**
   
   
   Subjects were seated in a sound-isolating booth on a chair in front of a 24-inch BenQ monitor with a viewing distance of approximately 60 cm. Stimuli were presented at an average level of 65 dB SPL and a sampling rate of 48000 Hz through ER-2 insert earphones plugged into an RME Babyface Pro digital sound card. The stimulus presentation for the experiment was controlled by a python script using a custom package, ``expyfun``.


Dataset Information
-------------------

.. list-table::
   :widths: 25 75
   :header-rows: 0

   * - Dataset ID
     - ``DS004356``
   * - Title
     - Subcortical responses to music and speech are alike while cortical responses diverge
   * - Year
     - 2022
   * - Authors
     - Tong Shan, Madeline S. Cappelloni, Ross K. Maddox
   * - License
     - CC0
   * - Citation / DOI
     - `doi:10.18112/openneuro.ds004356.v2.2.1 <https://doi.org/10.18112/openneuro.ds004356.v2.2.1>`__
   * - Source links
     - `OpenNeuro <https://openneuro.org/datasets/ds004356>`__ | `NeMAR <https://nemar.org/dataexplorer/detail?dataset_id=ds004356>`__ | `Source URL <https://openneuro.org/datasets/ds004356/versions/2.2.1>`__

.. dropdown:: Copy-paste BibTeX
   :class-container: sd-shadow-sm
   :class-title: sd-bg-light

   .. code-block:: bibtex

      @dataset{ds004356,
        title = {Subcortical responses to music and speech are alike while cortical responses diverge},
        author = {Tong Shan and Madeline S. Cappelloni and Ross K. Maddox},
        doi = {10.18112/openneuro.ds004356.v2.2.1},
        url = {https://doi.org/10.18112/openneuro.ds004356.v2.2.1},
      }

.. admonition:: Found an issue with this dataset?
   :class: tip

   If you encounter any problems with this dataset (missing files, incorrect metadata,
   loading errors, etc.), please let us know!

   .. button-link:: https://github.com/eegdash/EEGDash/issues/new?title=%5BDataset%5D%20Issue%20with%20DS004356&body=%23%23%20Dataset%0A%0A-%20%2A%2ADataset%20ID%3A%2A%2A%20DS004356%0A-%20%2A%2ATitle%3A%2A%2A%20Subcortical%20responses%20to%20music%20and%20speech%20are%20alike%20while%20cortical%20responses%20diverge%0A%0A%23%23%20Issue%20Description%0A%0APlease%20describe%20the%20issue%20you%20encountered%20with%20this%20dataset%3A%0A%0A%23%23%20Steps%20to%20Reproduce%0A%0A1.%20%0A2.%20%0A3.%20%0A%0A%23%23%20Expected%20Behavior%0A%0A%0A%23%23%20Additional%20Context%0A%0A&labels=dataset
      :color: primary
      :outline:

      Report an Issue on GitHub

Technical Details
-----------------

.. grid:: 1 2 3 3
   :gutter: 2

   .. grid-item-card:: Subjects & recordings
      :class-card: sd-border-1 highlight-primary

      - Subjects: 22
      - Recordings: 639
      - Tasks: 1

   .. grid-item-card:: Channels & sampling rate
      :class-card: sd-border-1 highlight-secondary

      - Channels: 34
      - Sampling rate (Hz): 10000.0
      - Duration (hours): 0.0

   .. grid-item-card:: Tags
      :class-card: sd-border-1 highlight-tertiary

      - Pathology: Not specified
      - Modality: —
      - Type: —

   .. grid-item-card:: Files & format
      :class-card: sd-border-1

      - Size on disk: 213.1 GB
      - File count: 639
      - Format: BIDS

   .. grid-item-card:: License & citation
      :class-card: sd-border-1

      - License: CC0
      - DOI: doi:10.18112/openneuro.ds004356.v2.2.1

   .. grid-item-card:: Provenance
      :class-card: sd-border-1

      - Source: openneuro
      - OpenNeuro: `ds004356 <https://openneuro.org/datasets/ds004356>`__
      - NeMAR: `ds004356 <https://nemar.org/dataexplorer/detail?dataset_id=ds004356>`__

API Reference
-------------

Use the ``DS004356`` class to access this dataset programmatically.

.. currentmodule:: eegdash.dataset

.. autoclass:: eegdash.dataset.DS004356
   :members: __init__, save
   :show-inheritance:
   :member-order: bysource


See Also
--------

* :class:`eegdash.dataset.EEGDashDataset`
* :mod:`eegdash.dataset`
* `OpenNeuro dataset page <https://openneuro.org/datasets/ds004356>`__
* `NeMAR dataset page <https://nemar.org/dataexplorer/detail?dataset_id=ds004356>`__

