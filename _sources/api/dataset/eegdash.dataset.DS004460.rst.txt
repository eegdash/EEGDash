..
   This documentation page is generated during the Sphinx build.
   The underlying code is manually maintained and not autogenerated.

eegdash.dataset.DS004460
========================

EEG and motion capture data set for a full-body/joystick rotation task (OpenNeuro ``ds004460``). Access recordings and metadata through EEGDash.

**Citation:** Gramann, K., Hohlefeld, F.U., Gehrke, L., Klug, M (2023). *EEG and motion capture data set for a full-body/joystick rotation task*. `10.18112/openneuro.ds004460.v1.1.0 <https://doi.org/10.18112/openneuro.ds004460.v1.1.0>`__

.. rst-class:: sd-badges

:bdg-primary:`Modality: eeg` :bdg-info:`Tasks: 1` :bdg-secondary:`Subjects: 20` :bdg-secondary:`Recordings: 346`

.. rst-class:: sd-badges

:bdg-success:`License: CC0` :bdg-warning:`Source: openneuro` :bdg-info:`Citations: 1.0`

Dataset Information
-------------------

.. list-table::
   :widths: 25 75
   :header-rows: 0

   * - Dataset ID
     - ``DS004460``
   * - Title
     - EEG and motion capture data set for a full-body/joystick rotation task
   * - Year
     - 2023
   * - Authors
     - Gramann, K., Hohlefeld, F.U., Gehrke, L., Klug, M
   * - License
     - CC0
   * - Citation / DOI
     - `doi:10.18112/openneuro.ds004460.v1.1.0 <https://doi.org/10.18112/openneuro.ds004460.v1.1.0>`__
   * - Source links
     - `OpenNeuro <https://openneuro.org/datasets/ds004460>`__ | `NeMAR <https://nemar.org/dataexplorer/detail?dataset_id=ds004460>`__ | `Source URL <https://openneuro.org/datasets/ds004460/versions/1.1.0>`__

.. dropdown:: Copy-paste BibTeX
   :class-container: sd-shadow-sm
   :class-title: sd-bg-light

   .. code-block:: bibtex

      @dataset{ds004460,
        title = {EEG and motion capture data set for a full-body/joystick rotation task},
        author = {Gramann, K. and Hohlefeld, F.U. and Gehrke, L. and Klug, M},
        doi = {10.18112/openneuro.ds004460.v1.1.0},
        url = {https://doi.org/10.18112/openneuro.ds004460.v1.1.0},
      }

.. admonition:: Found an issue with this dataset?
   :class: tip

   If you encounter any problems with this dataset (missing files, incorrect metadata,
   loading errors, etc.), please let us know!

   .. button-link:: https://github.com/eegdash/EEGDash/issues/new?title=%5BDataset%5D%20Issue%20with%20DS004460&body=%23%23%20Dataset%0A%0A-%20%2A%2ADataset%20ID%3A%2A%2A%20DS004460%0A-%20%2A%2ATitle%3A%2A%2A%20EEG%20and%20motion%20capture%20data%20set%20for%20a%20full-body/joystick%20rotation%20task%0A%0A%23%23%20Issue%20Description%0A%0APlease%20describe%20the%20issue%20you%20encountered%20with%20this%20dataset%3A%0A%0A%23%23%20Steps%20to%20Reproduce%0A%0A1.%20%0A2.%20%0A3.%20%0A%0A%23%23%20Expected%20Behavior%0A%0A%0A%23%23%20Additional%20Context%0A%0A&labels=dataset
      :color: primary
      :outline:

      Report an Issue on GitHub

Description
-----------

An EEG + motion capture data set, analyzed and published in "Gramann, K., Hohlefeld, F. U., Gehrke, L., & Klug, M. (2021). Human cortical dynamics during full-body heading changes. Scientific Reports, 11(1), 18186".

Used as a BIDS-example data set for EEG + motion : https://github.com/bids-standard/bids-examples/tree/master/motion_spotrotation


**Overview**

This is the "Spot rotation" dataset.
It contains EEG and motion data collected from 20 subjects
collected at the Berlin Mobile Brain-Body Imaging Lab,

.. dropdown:: View full README
   :class-container: sd-shadow-sm

   An EEG + motion capture data set, analyzed and published in "Gramann, K., Hohlefeld, F. U., Gehrke, L., & Klug, M. (2021). Human cortical dynamics during full-body heading changes. Scientific Reports, 11(1), 18186".
   
   Used as a BIDS-example data set for EEG + motion : https://github.com/bids-standard/bids-examples/tree/master/motion_spotrotation
   
   
   **Overview**
   
   This is the "Spot rotation" dataset.
   It contains EEG and motion data collected from 20 subjects
   collected at the Berlin Mobile Brain-Body Imaging Lab,
   while they rotated their heading in physical space or on flat screen using a joystick.
   Detailed description of the paradigm can be found in the following reference:
   
   Gramann.K, Hohlefeld, F. U., Gehrke, L., and Klug, M. 
   "Human cortical dynamics during full-body heading changes".
   Scientific Reports 11, 18186 (2021). 
   https://doi.org/10.1038/s41598-021-97749-8
   
   
   
   
   **Citing this dataset**
   
   Please cite as follows:
   
   Gramann, K., Hohlefeld, F.U., Gehrke, L. et al. Human cortical dynamics during full-body heading changes. Sci Rep 11, 18186 (2021). https://doi.org/10.1038/s41598-021-97749-8
   For more information, see the `dataset_description.json` file.
   
   
   
   **License**
   
   This motion_spotrotation dataset is made available under the Creative Commons CC0 license. 
   Information on CC0 can be found here : https://creativecommons.org/share-your-work/public-domain/cc0/
   
   
   
   **Format**
   
   The dataset is formatted according to the Brain Imaging Data Structure. See the
   `dataset_description.json` file for the specific version used.
   
   Generally, you can find data in the .tsv files and descriptions in the
   accompanying .json files.
   
   An important BIDS definition to consider is the "Inheritance Principle", which
   is described in the BIDS specification under the following link:
   
   https://bids-specification.rtfd.io/en/stable/02-common-principles.html#the-inheritance-principle
   
   The section states that:
   
   > Any metadata file (such as .json, .bvec or .tsv) may be defined at any directory level,
   > but no more than one applicable file may be defined at a given level [...]
   > The values from the top level are inherited by all lower levels unless
   > they are overridden by a file at the lower level.
   
   
   
   **Details about the experiment**
   
   For a detailed description of the task, see Gramann et al. (2021).
   What follows is a brief summary.
   
   Data were collected from 20 healthy adults (11 females) with a mean age of 30.25 years 
   (SD = 7.68, ranging from ages 20 to 46) who received 10€/h or course credit for compensation. 
   All participants reported normal or corrected to normal vision and no history of neurological disease. 
   Eighteen participants reported being right-handed (two left-handed). 
   
   To control for the effects of different reference frame proclivities on neural dynamics, 
   the online version of the spatial reference frame proclivity test (RFPT44, 45) 
   was administered prior to the experiment. 
   Participants had to consistently use an ego- or allocentric reference frame 
   in at least 80% of their responses. 
   Of the 20 participants, nine preferentially used an egocentric reference frame, 
   nine used an allocentric reference frame, and two used a mixed strategy. 
   One participant (egocentric reference frame) dropped out of the experiment 
   after the first block due to motion sickness and was removed from further data analyses. 
   The reported results are based on the remaining 19 participants. 
   The experimental procedures were approved by the local ethics committee 
   (Technische Universität Berlin, Germany) 
   and the research was performed in accordance with the ethics guidelines. 
   The study was conducted in accordance to the Declaration of Helsinki 
   and all participants signed a written informed consent. 
   
   Participants performed a spatial orientation task in a sparse virtual environment 
   (WorldViz Vizard, Santa Barbara, USA) consisting of an infinite floor granulated in green and black.
   The experiment was self-paced and participants advanced the experiment 
   by starting and ending each trial with a button press using the index finger of the dominant hand.
   A trial started with the onset of a red pole, which participants had to face and align with.
   Once the button was pressed the pole disappeared 
   and was immediately replaced by a red sphere floating at eye level. 
   The sphere automatically started to move around the participant 
   along a circular trajectory at a fixed distance (30 m) 
   with one of two different velocity profiles. 
   Participants were asked to rotate on the spot and to follow the sphere, 
   keeping it in the center of their visual field (outward rotation). 
   The sphere stopped unpredictably at varying eccentricity between 30° and 150° and turned blue, 
   which indicated that participants had to rotate back to the initial heading (backward rotation). 
   When participants had reproduced their estimated initial heading, 
   they confirmed their heading with a button press and the red pole reappeared for reorientation.
   
   The participants completed the experimental task twice, 
   using (i) a traditional desktop 2D setup (visual flow controlled through joystick movement; “joyR”), 
   and (ii) equipped with a MoBI setup 
   (visual flow controlled through active physical rotation with the whole body; “physR”). 
   The condition order was balanced across participants. 
   To ensure the comparability of both rotation conditions, 
   participants carried the full motion capture system at all times. 
   In the joyR condition participants stood in the dimly lit experimental hall in front of a standard TV monitor 
   (1.5 m viewing distance, HD resolution, 60 Hz refresh rate, 40″ diagonal size) 
   and were instructed to move as little as possible. 
   They followed the sphere by tilting the joystick 
   and were thus only able to use visual flow information to complete the task. 
   In the physical rotation condition participants were situated in a 3D virtual reality environment 
   using a head mounted display (HTC Vive; 2 × 1080 × 1200 resolution, 90 Hz refresh rate, 110° field of view). 
   Participants’ movements were unconstrained, 
   i.e., in order to follow the sphere they physically rotated on the spot, 
   thus enabling them to use motor and kinesthetic information (i.e., vestibular input and proprioception) 
   in addition to the visual flow for completing the task. 
   If participants diverged from the center position as determined through motion capture of the head position, 
   the task automatically halted and participants were asked to regain center position, 
   indicated by a yellow floating sphere, before continuing with the task. 
   Each movement condition was preceded by recording a three-minute baseline, 
   during which the participants were instructed to stand still and to look straight ahead.
   
   Data Recordings: EEG. 
   EEG data was recorded from 157 active electrodes with a sampling rate of 1000 Hz 
   and band-pass filtered from 0.016 Hz to 500 Hz (BrainAmp Move System, Brain Products, Gilching, Germany). 
   Using an elastic cap with an equidistant design (EASYCAP, Herrsching, Germany), 
   129 electrodes were placed on the scalp, and 28 electrodes were placed around the neck 
   using a custom neckband (EASYCAP, Herrsching, Germany) in order to record neck muscle activity. 
   Data were referenced to an electrode located closest to the standard position FCz. 
   Impedances were kept below 10kΩ for standard locations on the scalp, and below 50kΩ for the neckband. 
   Electrode locations were digitized using an optical tracking system (Polaris Vicra, NDI, Waterloo, ON, Canada).
   
   Data Recordings: Motion Capture. 
   Two different motion capture data sources were used: 19 red active light-emitting diodes (LEDs) were captured 
   using 31 cameras of the Impulse X2 System (PhaseSpace Inc., San Leandro, CA, USA) with a sampling rate of 90 Hz. 
   They were placed on the feet (2 x 4 LEDs), around the hips (5 LEDs), on the shoulders (4 LEDs), 
   and on the HTC Vive (2 LEDs; to account for an offset in yaw angle between the PhaseSpace and the HTC Vive tracking). 
   Except for the two LEDs on the HTC Vive, they were subsequently grouped together 
   to form rigid body parts of feet, hip, and shoulders, enabling tracking with 
   six degrees of freedom (x, y, and z position and roll, yaw, and pitch orientation) per body part. 
   Head motion capture data (position and orientation) was acquired using the HTC Lighthouse tracking system 
   with 90Hz sampling rate, since it was also used for the positional tracking of the virtual reality view. 
   
   The original data was recorded in `.xdf` format using labstreaminglayer
   (https://github.com/sccn/labstreaminglayer). It is stored in the `/sourcedata`
   directory. To comply with the BIDS format, the .xdf format was converted to
   BrainVision format (see the `.eeg` file for binary eeg data, the `.vhdr` as a
   text header filer containing meta data, and the `.vmrk` as a text file storing
   the eeg markers).


Highlights
----------

.. grid:: 1 2 3 3
   :gutter: 2

   .. grid-item-card:: Subjects & recordings
      :class-card: sd-border-1

      - Subjects: 20
      - Recordings: 346
      - Tasks: 1

   .. grid-item-card:: Channels & sampling rate
      :class-card: sd-border-1

      - Channels: 160
      - Sampling rate (Hz): 1000.0
      - Duration (hours): 0.0

   .. grid-item-card:: Tags
      :class-card: sd-border-1

      - Pathology: —
      - Modality: —
      - Type: —

   .. grid-item-card:: Files & format
      :class-card: sd-border-1

      - Size on disk: 59.1 GB
      - File count: 346
      - Format: BIDS

   .. grid-item-card:: License & citation
      :class-card: sd-border-1

      - License: CC0
      - DOI: doi:10.18112/openneuro.ds004460.v1.1.0

   .. grid-item-card:: Provenance
      :class-card: sd-border-1

      - Source: openneuro
      - OpenNeuro: `ds004460 <https://openneuro.org/datasets/ds004460>`__
      - NeMAR: `ds004460 <https://nemar.org/dataexplorer/detail?dataset_id=ds004460>`__

Quickstart
----------

**Install**

.. code-block:: bash

    pip install eegdash

**Access the data**

.. code-block:: python

    from eegdash.dataset import DS004460

    dataset = DS004460(cache_dir="./data")
    # Get the raw object of the first recording
    raw = dataset.datasets[0].raw
    print(raw.info)

**Filter/query**

.. tab-set::

   .. tab-item:: Basic

      .. code-block:: python

         dataset = DS004460(cache_dir="./data", subject="01")

   .. tab-item:: Advanced

      .. code-block:: python

         dataset = DS004460(
             cache_dir="./data",
             query={"subject": {"$in": ["01", "02"]}},
         )


Quality & caveats
-----------------

- No dataset-specific caveats are listed in the available metadata.

API
---

.. currentmodule:: eegdash.dataset

.. autoclass:: eegdash.dataset.DS004460
   :members: __init__, save
   :show-inheritance:
   :member-order: bysource


See Also
--------

* :class:`eegdash.dataset.EEGDashDataset`
* :mod:`eegdash.dataset`
* `OpenNeuro dataset page <https://openneuro.org/datasets/ds004460>`__
* `NeMAR dataset page <https://nemar.org/dataexplorer/detail?dataset_id=ds004460>`__

