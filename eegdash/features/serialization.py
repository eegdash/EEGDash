r"""
Serialization Utilities for Feature Datasets.

This module provides functions for reconstructing feature datasets 
from disk. It serves as the inverse of the saving logic implemented in 
:class:`FeaturesConcatDataset`, allowing for efficient, parallelized 
reloading of processed features and their associated metadata.

See Also
--------
https://github.com/braindecode/braindecode/blob/master/braindecode/datautil/serialization.py#L165-L229

"""

from __future__ import annotations

from pathlib import Path

import pandas as pd
from joblib import Parallel, delayed
from mne.io import read_info

from braindecode.datautil.serialization import _load_kwargs_json

from .datasets import FeaturesConcatDataset, FeaturesDataset

__all__ = [
    "load_features_concat_dataset",
]


def load_features_concat_dataset(
    path: str | Path, ids_to_load: list[int] | None = None, n_jobs: int = 1
) -> FeaturesConcatDataset:
    r"""Load a stored :class:`FeaturesConcatDataset` from a directory.

    This function reconstructs a concatenated dataset by loading individual 
    :class:`FeaturesDataset` instances from numbered subdirectories.

    Parameters
    ----------
    path : str or pathlib.Path
        The root directory where the dataset was previously saved. This 
        directory should contain numbered subdirectories.
    ids_to_load : list of int, optional
        A list of specific recording IDs (subdirectory names) to load. 
        If **None**, all numbered subdirectories found in the path are 
        loaded in ascending numerical order.
    n_jobs : int, default=1
        The number of CPU cores to use for parallel loading. Set to -1 to 
        use all available processors.

    Returns
    -------
    FeaturesConcatDataset
        A unified concatenated dataset containing the loaded recordings.

    Notes
    -----
    The function expects the directory structure generated by 
    :meth:`FeaturesConcatDataset.save`. It automatically reconstructs 
    the feature DataFrames (Parquet), metadata (Pickle), recording info 
    (FIF), and preprocessing keyword arguments (JSON).

    """
    # Verify work with a pathlib.Path
    path = Path(path)

    if ids_to_load is None:
        # Get all subdirectories and sort them numerically
        ids_to_load = [p.name for p in path.iterdir() if p.is_dir()]
        ids_to_load = sorted(ids_to_load, key=lambda i: int(i))
    ids_to_load = [str(i) for i in ids_to_load]

    datasets = Parallel(n_jobs)(delayed(_load_parallel)(path, i) for i in ids_to_load)
    return FeaturesConcatDataset(datasets)


def _load_parallel(path: Path, i: str) -> FeaturesDataset:
    r"""Load a single :class:`FeaturesDataset` from its subdirectory.

    Internal helper function for :func:`load_features_concat_dataset`.

    Parameters
    ----------
    path : pathlib.Path
        The root path of the saved concatenated collection.
    i : str
        The identifier of the dataset (directory name) to load.

    Returns
    -------
    FeaturesDataset
        The reconstructed recording-level dataset instance.

    Notes
    -----
    This helper looks for files following the standard naming convention:
    - ``{i}-feat.parquet``: Feature values.
    - ``metadata_df.pkl``: Sample-level metadata.
    - ``description.json``: Dataset-level description.
    - ``raw-info.fif``: Original recording info (if available).
    
    """
    sub_dir = path / i

    parquet_name_pattern = "{}-feat.parquet"
    parquet_file_name = parquet_name_pattern.format(i)
    parquet_file_path = sub_dir / parquet_file_name

    features = pd.read_parquet(parquet_file_path)

    description_file_path = sub_dir / "description.json"
    description = pd.read_json(description_file_path, typ="series")

    raw_info_file_path = sub_dir / "raw-info.fif"
    raw_info = None
    if raw_info_file_path.exists():
        raw_info = read_info(raw_info_file_path)

    raw_preproc_kwargs = _load_kwargs_json("raw_preproc_kwargs", sub_dir)
    window_kwargs = _load_kwargs_json("window_kwargs", sub_dir)
    window_preproc_kwargs = _load_kwargs_json("window_preproc_kwargs", sub_dir)
    features_kwargs = _load_kwargs_json("features_kwargs", sub_dir)
    metadata = pd.read_pickle(path / i / "metadata_df.pkl")

    dataset = FeaturesDataset(
        features,
        metadata=metadata,
        description=description,
        raw_info=raw_info,
        raw_preproc_kwargs=raw_preproc_kwargs,
        window_kwargs=window_kwargs,
        window_preproc_kwargs=window_preproc_kwargs,
        features_kwargs=features_kwargs,
    )
    return dataset
