# Reusable workflow for cloning and digesting datasets
name: Clone & Digest (Reusable)

on:
  workflow_call:
    inputs:
      source_name:
        description: 'Name of the source (e.g., openneuro, nemar)'
        required: true
        type: string
      consolidated_file:
        description: 'Consolidated JSON file to read from'
        required: true
        type: string
      max_datasets:
        description: 'Maximum datasets to process (0 = all)'
        required: false
        type: number
        default: 0
      workers:
        description: 'Number of parallel workers'
        required: false
        type: number
        default: 4
    secrets:
      DATASET_LISTINGS_TOKEN:
        required: true
    outputs:
      datasets_processed:
        description: 'Number of datasets processed'
        value: ${{ jobs.digest.outputs.datasets_processed }}
      records_created:
        description: 'Number of records created'
        value: ${{ jobs.digest.outputs.records_created }}

jobs:
  digest:
    name: Clone & Digest ${{ inputs.source_name }}
    runs-on: ubuntu-latest
    outputs:
      datasets_processed: ${{ steps.summary.outputs.datasets }}
      records_created: ${{ steps.summary.outputs.records }}

    steps:
      - name: Checkout EEGDash
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          ref: ${{ github.head_ref }}
          fetch-depth: 0
          path: eegdash

      - name: Checkout dataset listings repository
        uses: actions/checkout@v4
        with:
          repository: eegdash/eegdash-dataset-listings
          token: ${{ secrets.DATASET_LISTINGS_TOKEN }}
          persist-credentials: true
          fetch-depth: 1
          path: eegdash-dataset-listings

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'eegdash/pyproject.toml'

      - name: Install dependencies
        run: |
          cd eegdash
          python -m pip install --upgrade pip
          pip install -e .[digestion]

      - name: Create directories
        run: |
          mkdir -p eegdash-dataset-listings/cloned
          mkdir -p eegdash-dataset-listings/digested

      - name: Smart Clone datasets
        run: |
          ARGS="--input eegdash-dataset-listings/consolidated/${{ inputs.consolidated_file }}"
          ARGS="$ARGS --output eegdash-dataset-listings/cloned"
          ARGS="$ARGS --workers ${{ inputs.workers }}"
          
          if [ "${{ inputs.max_datasets }}" != "0" ]; then
            ARGS="$ARGS --limit ${{ inputs.max_datasets }}"
          fi
          
          python eegdash/scripts/ingestions/2_clone.py $ARGS || true

      - name: Digest datasets
        run: |
          # Only digest if there are cloned datasets
          if [ -d "eegdash-dataset-listings/cloned" ] && [ "$(ls -A eegdash-dataset-listings/cloned 2>/dev/null)" ]; then
            ARGS="--input eegdash-dataset-listings/cloned"
            ARGS="$ARGS --output eegdash-dataset-listings/digested"
            ARGS="$ARGS --workers ${{ inputs.workers }}"
            
            python eegdash/scripts/ingestions/3_digest.py $ARGS || true
          else
            echo "No datasets to digest"
          fi

      - name: Generate summary
        id: summary
        run: |
          cd eegdash-dataset-listings/digested
          
          # Count datasets and records
          DATASETS=$(find . -name "*_dataset.json" | wc -l | tr -d ' ')
          RECORDS=$(python -c "
          import json, glob
          total = 0
          for f in glob.glob('*/*_records.json'):
              try:
                  data = json.load(open(f))
                  total += data.get('record_count', len(data.get('records', [])))
              except: pass
          print(total)
          ")
          
          echo "datasets=$DATASETS" >> $GITHUB_OUTPUT
          echo "records=$RECORDS" >> $GITHUB_OUTPUT
          
          echo "### ðŸ“Š ${{ inputs.source_name }} Digest Results" >> $GITHUB_STEP_SUMMARY
          echo "- Datasets processed: $DATASETS" >> $GITHUB_STEP_SUMMARY
          echo "- Records created: $RECORDS" >> $GITHUB_STEP_SUMMARY

      - name: Upload digested artifacts
        uses: actions/upload-artifact@v4
        with:
          name: digested-${{ inputs.source_name }}
          path: eegdash-dataset-listings/digested/
          retention-days: 7

      - name: Commit digested data
        run: |
          cd eegdash-dataset-listings
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          # Add digested files
          git add digested/ || true
          
          if ! git diff --cached --quiet; then
            git commit -m "chore: digest ${{ inputs.source_name }} datasets [$(date -u +%Y-%m-%d)]"
            git pull --rebase origin main
            git push
          fi
