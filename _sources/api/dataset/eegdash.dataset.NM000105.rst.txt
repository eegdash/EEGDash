..
   This documentation page is generated during the Sphinx build.
   The underlying code is manually maintained and not autogenerated.

:html_theme.sidebar_secondary.remove:

NM000105
========

*discrete_gestures: Discrete Hand Gesture Detection from EMG*

Access recordings and metadata through EEGDash.

**Citation:** Patrick Kaifosh, Thomas R. Reardon, CTRL-labs at Reality Labs (2025). *discrete_gestures: Discrete Hand Gesture Detection from EMG*. `10.5281/zenodo.17283593 <https://doi.org/10.5281/zenodo.17283593>`__

.. rst-class:: sd-badges

:bdg-primary-line:`Modality: emg` :bdg-secondary-line:`Subjects: 100` :bdg-secondary-line:`Recordings: 100` :bdg-success-line:`License: CC-BY-NC 4.0` :bdg-warning-line:`Source: nemar`

:bdg-success:`Metadata: Complete (100%)`

Quickstart
----------

.. tab-set::

   .. tab-item:: Get Started
      :sync: start

      **Install**

      .. code-block:: bash

         pip install eegdash

      **Access the data**

      .. code-block:: python

         from eegdash.dataset import NM000105

         dataset = NM000105(cache_dir="./data")
         # Get the raw object of the first recording
         raw = dataset.datasets[0].raw
         print(raw.info)

   .. tab-item:: Query & Filter
      :sync: query

      **Filter by subject**

      .. code-block:: python

         dataset = NM000105(cache_dir="./data", subject="01")

      **Advanced query**

      .. code-block:: python

         dataset = NM000105(
             cache_dir="./data",
             query={"subject": {"$in": ["01", "02"]}},
         )

      **Iterate recordings**

      .. code-block:: python

         for rec in dataset:
             print(rec.subject, rec.raw.info['sfreq'])

   .. tab-item:: Cite This Dataset
      :sync: cite

      If you use this dataset in your research, please cite the original authors.

      **BibTeX**

      .. code-block:: bibtex

         @dataset{nm000105,
           title = {discrete_gestures: Discrete Hand Gesture Detection from EMG},
           author = {Patrick Kaifosh and Thomas R. Reardon and CTRL-labs at Reality Labs},
           doi = {10.5281/zenodo.17283593},
           url = {https://doi.org/10.5281/zenodo.17283593},
         }


About This Dataset
------------------


**discrete_gestures: Discrete Hand Gesture Detection from EMG**


**Overview**

**Dataset**: discrete_gestures - Discrete hand gestures from wrist-based surface electromyography
**Task**: Nine discrete hand gestures (pinches and swipes)
**Participants**: 100 subjects
**Sessions**: 100 total (1 per subject)

.. dropdown:: View full README
   :class-container: sd-shadow-sm

   
   **discrete_gestures: Discrete Hand Gesture Detection from EMG**
   
   
   **Overview**
   
   **Dataset**: discrete_gestures - Discrete hand gestures from wrist-based surface electromyography
   **Task**: Nine discrete hand gestures (pinches and swipes)
   **Participants**: 100 subjects
   **Sessions**: 100 total (1 per subject)
   **Publication**: Kaifosh et al., 2025 - "A generic non-invasive neuromotor interface for human-computer interaction" (Nature)
   
   **Purpose**
   
   This dataset captures wrist-based sEMG signals during prompted discrete hand gestures for navigation and activation tasks. The goal is to enable gesture-based computer control without cameras or visible hand movements, with applications in AR/VR, mobile interfaces, and accessibility.
   Key research objectives:
   - Generic models that work across users without calibration
   - Discrete gesture classification with high accuracy
   - Real-time gesture detection for interactive systems
   - Robustness to electrode placement variability
   
   **Dataset Details**
   
   
   **Participants**
   
   **Sample size**: 100 participants
   **Demographics**: Not available (age, sex, handedness marked as n/a)
   **Recording side**: Dominant wrist (assumed right-handed, varies by participant)
   **Sessions**: 1 session per participant
   
   **Hardware**
   
   **Device**: sEMG Research Device (sEMG-RD)
   **Configuration**: Single wristband (dominant wrist)
   **Channels**: 16
   **Sampling rate**: 2000 Hz
   **Bit depth**: 12 bits
   **Dynamic range**: ±6.6 mV
   **Bandwidth**: 20-850 Hz
   **Connectivity**: Bluetooth
   **Electrode type**: Dry gold-plated differential pairs
   
   **Gestures**
   
   **Nine discrete gestures**:
   **Thumb swipes** (4):
   - Left swipe
   - Right swipe
   - Up swipe
   - Down swipe
   
   **Pinches** (4):
   - Index-to-thumb pinch
   - Middle-to-thumb pinch
   - Ring-to-thumb pinch
   - Pinky-to-thumb pinch
   
   **Activation** (1):
   - Thumb tap
   
   **Recording Protocol**
   
   1. Participant dons sEMG-RD on dominant wrist
   2. Gesture prompter displays gesture cue (scrolling left-to-right)
   3. Participant performs prompted gesture
   4. Randomized order with randomized inter-gesture intervals
   5. Multiple repetitions of each gesture type
   
   **Session duration**: Varies by participant
   **Total gestures**: 1900 prompted gestures across all participants
   **Stage boundaries**: 16 recording stages per session
   
   **Data Contents**
   
   
   **Files per Session**
   
   
   .. code-block:: text
   
      sub-XXX/ses-XXX/emg/
   
   .. code-block:: text
   
         ├── sub-XXX_ses-XXX_task-discretegestures_emg.edf
         ├── sub-XXX_ses-XXX_task-discretegestures_emg.json
         ├── sub-XXX_ses-XXX_task-discretegestures_channels.tsv
         ├── sub-XXX_ses-XXX_task-discretegestures_events.tsv
         └── sub-XXX_ses-XXX_electrodes.tsv
   
      
   
   
   **Channel Configuration**
   
   **Total channels**: 16 (EMG0-EMG15)
   **Channel naming**: Unique identifiers (EMG0-EMG15)
   **Electrode naming**: E0-E15 (physical positions)
   **Reference**: Bipolar (differential sensing)
   **channels.tsv columns**:
   - ``name``: Channel identifier (EMG0-EMG15)
   - ``type``: EMG
   - ``units``: V
   - ``signal_electrode``: Physical electrode name (E0-E15)
   - ``reference``: bipolar
   
   **electrodes.tsv columns**:
   - ``name``: Electrode identifier (E0-E15)
   - ``x``, ``y``, ``z``: 3D coordinates (percent units, no decimals)
   
   **Events**
   
   **events.tsv contains**:
   - **Gesture prompts**: Timestamped prompts for each gesture
     - ``type``: gesture_X (where X is the gesture name)
     - ``latency``: Sample index when gesture was prompted
     - ``gesture_type``: Specific gesture (e.g., "index_pinch", "thumb_swipe_left")
   - **Stage boundaries**: Recording session phases
     - ``type``: stage_boundary
     - ``stage_name``: Stage identifier
   
   **Total events**: 1916 (1900 gesture prompts + 16 stage boundaries)
   
   **Coordinate System**
   
   **Single coordinate system** (no space entity):
   
   .. code-block:: text
   
      EMGCoordinateSystem: Other
      EMGCoordinateUnits: percent
      X: USP → RSP (0-100%)
      Y: Right-hand rule perpendicular (0-100%)
      Z: Radial offset (constant 10%)
      
   
   **Anatomical landmarks**:
   - RSP: Radial Styloid Process
   - USP: Ulnar Styloid Process
   
   **Note**: Right-handed coordinate system for dominant wrist
   
   **Signal Processing**
   
   
   **Preprocessing Applied**
   
   1. **High-pass filtering**: 40 Hz cutoff
   2. **Clock drift correction**: Time synchronization
   3. **Irregular sampling handling**: Resampling when deviation >1% (up to 9290% deviation detected)
   
   **Signal Characteristics**
   
   **Gesture patterns**:
   - Patterned activity across channels corresponding to flexor/extensor muscles
   - Fine differences across gesture instances
   - Channel activity correlates with muscle positions (Fig. 1 in paper)
   
   **Baseline Performance**
   
   
   **Published Results (Kaifosh et al., 2025)**
   
   **Offline Classification** (held-out participants):
   - Accuracy: >90% for gesture classification
   - False-negative rate improves with more training data
   - Generic models trained on hundreds of participants
   
   **Closed-loop Performance** (n=24 naive test users):
   - **First-hit probability**: Median improvement from 0.74 (practice) to 0.82 (evaluation block 2)
   - **Gesture completion rate**: Median 0.88 gestures/second (evaluation block 2)
   - **Baseline comparison**: Gaming controller achieves 1.45 completions/second
   
   **Model architecture**: 1D convolution → LSTM layers
   **Learning effects**: Participants improve from practice to evaluation blocks
   
   **Representation Analysis**
   
   **Network learns**:
   - First layer filters resemble motor unit action potentials (MUAPs)
   - Deeper layers progressively separate gesture categories
   - Invariance to nuisance variables (participant ID, electrode placement, signal power)
   
   **Confusion Matrix**
   
   **Common confusions** (from paper):
   - Index and middle holds sometimes released too early
   - Similar gestures (e.g., adjacent finger pinches) occasionally confused
   - Swipe directions generally well-separated
   
   **Note**: Some errors are behavioral (wrong gesture performed) not just decoding errors
   
   **Use Cases**
   
   
   **Machine Learning**
   
   - **Time series classification**: Discrete event detection
   - **Generic modeling**: Out-of-the-box cross-user generalization
   - **Representation learning**: Physiologically-grounded features
   - **Real-time prediction**: Low-latency gesture detection
   
   **Applications**
   
   - **Grid navigation**: Discrete movement in 2D space
   - **Menu selection**: Activation gestures for UI elements
   - **Game control**: Gesture-based game inputs
   - **AR/VR interfaces**: Hands-free navigation
   - **Accessibility**: Alternative input modality
   
   **Known Issues and Limitations**
   
   
   **By Design**
   
   - **Single wrist**: Dominant hand only (not bilateral)
   - **Handedness unknown**: Assumed right-handed, varies by participant
   - **Gesture novelty**: Users needed coaching to learn effective gestures
   - **No demographic data**: Age, sex, handedness not collected
   
   **Technical**
   
   - **Electrode placement**: Single session per user (less cross-session data than emg2qwerty)
   - **Signal amplitude**: Varies with gesture force
   - **Hardware unavailable**: sEMG-RD not commercially available
   
   **Data Quality**
   
   - **Irregular sampling**: High deviation detected (up to 9290%), resampling applied
   - **Behavioral errors**: Not all errors are decoder errors (some user mistakes)
   
   **Comparison to Baselines**
   
   **Nintendo Joy-Con controller**:
   - Median: 1.45 completions/second
   - sEMG decoder: 0.88 completions/second (66% slower)
   
   **However**: sEMG doesn't require hand-encumbering device
   
   **BIDS Format**
   
   
   .. code-block:: text
   
      Pernet, C.R., et al. (2019). EEG-BIDS, an extension to the brain
      imaging data structure for electroencephalography.
      Scientific Data, 6(1), 103.
      
   
   
   **Access and Contact**
   
   **Original data**: Part of Meta Reality Labs neuromotor interface research
   **BIDS conversion**: Custom MATLAB tools using EEGLAB BIDS plugin
   **Data curator**: Yahya Shirazi, SCCN (Swartz Center for Computational Neuroscience), INC (Institute for Neural Computation), UCSD
   **Contact**: See Nature paper for corresponding authors
   
   **License**
   
   Research and educational use. See original publication.
   
   **Citation**
   
   
   .. code-block:: text
   
      Kaifosh, P., Reardon, T.R., & CTRL-labs at Reality Labs. (2025).
      A generic non-invasive neuromotor interface for human-computer interaction.
      Nature, 645(8081), 702-711. https://doi.org/10.1038/s41586-025-09255-w
      
   
   
   **Data Curator**
   
   **Yahya Shirazi**
   SCCN (Swartz Center for Computational Neuroscience)
   INC (Institute for Neural Computation)
   University of California San Diego
   
   **Version History**
   
   
   **v1.0 (2025-10-01): Initial BIDS conversion**
   
   **BIDS Version**: 1.11 \| **EMG-BIDS**: BEP-042 \| **Updated**: Oct 1, 2025


Dataset Information
-------------------

.. list-table::
   :widths: 25 75
   :header-rows: 0

   * - Dataset ID
     - ``NM000105``
   * - Title
     - discrete_gestures: Discrete Hand Gesture Detection from EMG
   * - Year
     - 2025
   * - Authors
     - Patrick Kaifosh, Thomas R. Reardon, CTRL-labs at Reality Labs
   * - License
     - CC-BY-NC 4.0
   * - Citation / DOI
     - `10.5281/zenodo.17283593 <https://doi.org/10.5281/zenodo.17283593>`__
   * - Source links
     - `OpenNeuro <https://openneuro.org/datasets/nm000105>`__ | `NeMAR <https://nemar.org/dataexplorer/detail?dataset_id=nm000105>`__ | `Source URL <https://nemar.org/dataexplorer/detail/nm000105>`__

.. dropdown:: Copy-paste BibTeX
   :class-container: sd-shadow-sm
   :class-title: sd-bg-light

   .. code-block:: bibtex

      @dataset{nm000105,
        title = {discrete_gestures: Discrete Hand Gesture Detection from EMG},
        author = {Patrick Kaifosh and Thomas R. Reardon and CTRL-labs at Reality Labs},
        doi = {10.5281/zenodo.17283593},
        url = {https://doi.org/10.5281/zenodo.17283593},
      }

.. admonition:: Found an issue with this dataset?
   :class: tip

   If you encounter any problems with this dataset (missing files, incorrect metadata,
   loading errors, etc.), please let us know!

   .. button-link:: https://github.com/eegdash/EEGDash/issues/new?title=%5BDataset%5D%20Issue%20with%20NM000105&body=%23%23%20Dataset%0A%0A-%20%2A%2ADataset%20ID%3A%2A%2A%20NM000105%0A-%20%2A%2ATitle%3A%2A%2A%20discrete_gestures%3A%20Discrete%20Hand%20Gesture%20Detection%20from%20EMG%0A%0A%23%23%20Issue%20Description%0A%0APlease%20describe%20the%20issue%20you%20encountered%20with%20this%20dataset%3A%0A%0A%23%23%20Steps%20to%20Reproduce%0A%0A1.%20%0A2.%20%0A3.%20%0A%0A%23%23%20Expected%20Behavior%0A%0A%0A%23%23%20Additional%20Context%0A%0A&labels=dataset
      :color: primary
      :outline:

      Report an Issue on GitHub

Technical Details
-----------------

.. grid:: 1 2 3 3
   :gutter: 2

   .. grid-item-card:: Subjects & recordings
      :class-card: sd-border-1 highlight-primary

      - Subjects: 100
      - Recordings: 100
      - Tasks: 1

   .. grid-item-card:: Channels & sampling rate
      :class-card: sd-border-1 highlight-secondary

      - Channels: 16
      - Sampling rate (Hz): 2000.0
      - Duration (hours): 0.0

   .. grid-item-card:: Tags
      :class-card: sd-border-1 highlight-tertiary

      - Pathology: Healthy
      - Modality: Visual
      - Type: Motor

   .. grid-item-card:: Files & format
      :class-card: sd-border-1

      - Size on disk: 6.6 MB
      - File count: 100
      - Format: BIDS

   .. grid-item-card:: License & citation
      :class-card: sd-border-1

      - License: CC-BY-NC 4.0
      - DOI: 10.5281/zenodo.17283593

   .. grid-item-card:: Provenance
      :class-card: sd-border-1

      - Source: nemar
      - OpenNeuro: `nm000105 <https://openneuro.org/datasets/nm000105>`__
      - NeMAR: `nm000105 <https://nemar.org/dataexplorer/detail?dataset_id=nm000105>`__

API Reference
-------------

Use the ``NM000105`` class to access this dataset programmatically.

.. currentmodule:: eegdash.dataset

.. autoclass:: eegdash.dataset.NM000105
   :members: __init__, save
   :show-inheritance:
   :member-order: bysource


See Also
--------

* :class:`eegdash.dataset.EEGDashDataset`
* :mod:`eegdash.dataset`
* `OpenNeuro dataset page <https://openneuro.org/datasets/nm000105>`__
* `NeMAR dataset page <https://nemar.org/dataexplorer/detail?dataset_id=nm000105>`__

